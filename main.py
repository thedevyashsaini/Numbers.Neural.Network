# -*- coding: utf-8 -*-
"""Copy of NNN | Number's Neural Network

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wQjJkYjqBYhF80JKwA0W_JTurRiOh2xI
"""

# @title Mount Drive Folder and Set Dataset Path
dataset_folder = "Handwritten_Digits" # @param {"type":"string","placeholder":"Handwritten_Digits"}
import os
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

dataset_path = "/content/drive/My Drive/" + dataset_folder

# @title Define NN Architecture
layer_1 = 20 # @param {"type":"number","placeholder":"20"}
layer_2 = 20 # @param {"type":"number","placeholder":"20"}
layer_3 = 10 # @param {"type":"number","placeholder":"20"}
output_layer = 10 # @param {"type":"number","placeholder":"20"}
n = [100, layer_1, layer_2, layer_3, output_layer]
print("layer 0 / input layer size      :", n[0])
print("layer 1 size                    :", n[1])
print("layer 2 size                    :", n[2])
print("layer 3 size                    :", n[3])
print("layer 4 / output layer size     :", n[4])

# @title Function to Load Pre-trained Model
def load_model(file_name):
    global W1, W2, W3, W4, b1, b2, b3, b4

    try:
        with open(file_name, 'rb') as file:
            model_params = pickle.load(file)

        def convert(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert(v) for k, v in obj.items()}
            else:
                return obj

        model_params_json = convert(model_params)

        with open("model.json", "w") as newfile:
            json.dump(model_params_json, newfile)

        W1 = model_params['W1']
        W2 = model_params['W2']
        W3 = model_params['W3']
        W4 = model_params['W4']
        b1 = model_params['b1']
        b2 = model_params['b2']
        b3 = model_params['b3']
        b4 = model_params['b4']

        print("Model parameters loaded successfully.")
    except Exception as e:
        print(f"Error loading model: {e}")

# @title Initialise Random Weights
import numpy as np

def init_weights(n_inputs, n_outputs):
    return np.random.randn(n_outputs, n_inputs) * np.sqrt(2.0 / n_inputs)

W1 = init_weights(n[0], n[1])
W2 = init_weights(n[1], n[2])
W3 = init_weights(n[2], n[3])
W4 = init_weights(n[3], n[4])

b1 = np.zeros((n[1], 1))
b2 = np.zeros((n[2], 1))
b3 = np.zeros((n[3], 1))
b4 = np.zeros((n[4], 1))

print("Layer 1 weights' shape          :", W1.shape)
print("Layer 2 weights' shape          :", W2.shape)
print("Layer 3 weights' shape          :", W3.shape)
print("Layer 4 weights' shape          :", W4.shape)
print("Layer 1 bias' shape             :", b1.shape)
print("Layer 2 bias' shape             :", b2.shape)
print("Layer 3 bias' shape             :", b3.shape)
print("Layer 4 bias' shape             :", b4.shape)

import json, os
import numpy as np
from sklearn.model_selection import train_test_split
import random
import cv2

# === Configuration ===
# dataset_path = "/content/drive/MyDrive/YourDatasetFolder"  # Change to your folder
test_size = 0.1 # @param {"type":"number","placeholder":"Test Size"}
augment_target = 10000 # @param {"type" :"number","placeholder":"augment_target"}
random_state = 42

# === Data Augmentation Function ===

def augment_image(image, num_variants=5, noise_std=0.00, dropout_prob=0.1, brightness_factor=0.2, shift_pixels=1):
    img = np.array(image).reshape(10, 10)
    variants = [img.copy()]  # include original

    for _ in range(num_variants):
        variant = img.copy()

        # 1. Add Gaussian noise
        noise = np.random.normal(0, noise_std, variant.shape)
        variant = np.clip(variant + noise, 0, 1)

        # 2. Random dropout (pixel dropout)
        dropout_mask = np.random.rand(*variant.shape) > dropout_prob
        variant = variant * dropout_mask

        # 3. Brightness/contrast change
        brightness = np.random.uniform(1 - brightness_factor, 1 + brightness_factor)
        variant = np.clip(variant * brightness, 0, 1)

        # 4. Slight shift (translation)
        tx = np.random.randint(-shift_pixels, shift_pixels + 1)
        ty = np.random.randint(-shift_pixels, shift_pixels + 1)
        M = np.float32([[1, 0, tx], [0, 1, ty]])
        variant = cv2.warpAffine(variant, M, (10, 10), borderValue=0)

        variants.append(variant)

    return [v.flatten() for v in variants]

# === Main Function ===
def prepare_data(split=True):
    data_samples = []
    labels = []

    # Load all JSON files
    for file_name in os.listdir(dataset_path):
        if file_name.endswith('.json'):
            with open(os.path.join(dataset_path, file_name), 'r') as f:
                data = json.load(f)
                for entry in data:
                    data_samples.append(entry['data'])
                    labels.append(entry['label'])

    # Convert to numpy arrays
    data_samples = np.array(data_samples)
    labels = np.array(labels)

    # Shuffle BEFORE augmentation
    np.random.seed(random_state)
    indices = np.random.permutation(len(data_samples))
    data_samples = data_samples[indices]
    labels = labels[indices]

    # === Augment until augment_target reached ===
    augmented_data = []
    augmented_labels = []

    total = 0
    i = 0

    while total < augment_target:
        x = data_samples[i % len(data_samples)]
        y = labels[i % len(labels)]

        variants = augment_image(x)
        for v in variants:
            if total >= augment_target:
                break
            augmented_data.append(v)
            augmented_labels.append(y)
            total += 1

        i += 1

    # Shuffle AFTER augmentation
    augmented_data = np.array(augmented_data)
    augmented_labels = np.array(augmented_labels)

    np.random.seed(random_state)
    indices = np.random.permutation(len(augmented_data))
    augmented_data = augmented_data[indices]
    augmented_labels = augmented_labels[indices]

    if split:
        data_train, data_test, labels_train, labels_test = train_test_split(
            augmented_data, augmented_labels, test_size=test_size, random_state=random_state
        )

        m_train = len(data_train)
        A0 = data_train.T
        Y = np.zeros((10, m_train))
        Y[labels_train, np.arange(m_train)] = 1

        m_test = len(data_test)
        A0_test = data_test.T
        Y_test = np.zeros((10, m_test))
        Y_test[labels_test, np.arange(m_test)] = 1

        return A0, Y, m_train, A0_test, Y_test, m_test

    else:
        m = len(augmented_data)
        A0 = augmented_data.T
        Y = np.zeros((10, m))
        Y[augmented_labels, np.arange(m)] = 1
        return A0, Y, m

# @title Helper Formulae
# def sigmoid(Z):
#     return 1.0 / (1.0 + np.exp(-np.clip(Z, -500, 500)))

# def sigmoid_derivative(Z):
#     s = sigmoid(Z)
#     return s * (1 - s)

def leaky_relu(Z, alpha=0.01):
    return np.where(Z > 0, Z, alpha * Z)

def leaky_relu_derivative(Z, alpha=0.01):
    return np.where(Z > 0, 1, alpha)

def apply_dropout(A, dropout_rate):
    D = np.random.rand(*A.shape) > dropout_rate
    A_dropped = A * D
    A_dropped /= (1.0 - dropout_rate)
    return A_dropped, D


def softmax(Z):
    Z = np.clip(Z, -500, 500)  # Prevent overflow
    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))
    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)

def cross_entropy_loss(Y, Y_hat, weights=None, lambd=0.0):
    epsilon = 1e-15  # to avoid log(0)
    Y_hat = np.clip(Y_hat, epsilon, 1 - epsilon)
    m = Y.shape[1]

    # Cross-entropy part
    loss = -np.sum(Y * np.log(Y_hat)) / m

    # L2 regularization part
    if weights is not None and lambd > 0:
        l2_sum = sum(np.sum(W ** 2) for W in weights)
        loss += (lambd / (2 * m)) * l2_sum
    return loss


def accuracy(Y, Y_hat):
    Y_pred = np.argmax(Y_hat, axis=0)
    Y_true = np.argmax(Y, axis=0)

    correct_predictions = np.sum(Y_pred == Y_true)
    total_predictions = Y.shape[1]

    return correct_predictions / total_predictions

# @title Prediction Function
def feed_forward(A0, training=False, dropout_rate=0.3):
    Z1 = W1 @ A0 + b1
    A1 = leaky_relu(Z1)

    D1 = None
    if training:
        A1, D1 = apply_dropout(A1, dropout_rate)

    Z2 = W2 @ A1 + b2
    A2 = leaky_relu(Z2)

    D2 = None
    if training:
        A2, D2 = apply_dropout(A2, dropout_rate)

    Z3 = W3 @ A2 + b3
    A3 = leaky_relu(Z3)

    D3 = None
    if training:
        A3, D3 = apply_dropout(A3, dropout_rate)

    Z4 = W4 @ A3 + b4
    A4 = softmax(Z4)

    cache = {
        "Z1": Z1, "A1": A1, "D1": D1,
        "Z2": Z2, "A2": A2, "D2": D2,
        "Z3": Z3, "A3": A3, "D3": D3,
        "Z4": Z4, "A4": A4
    }

    return A4, cache

def backprop_layer_4(Y_hat, Y, m, A2, W3, lambd=0.0):
    # Output layer backprop (softmax + cross-entropy)
    dZ3 = Y_hat - Y  # shape: (num_classes, m)
    dW3 = (1/m) * (dZ3 @ A2.T) + (lambd/m) * W3  # L2 term added
    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)
    dA2 = W3.T @ dZ3
    return dW3, db3, dA2


def backprop_hidden_layer(dA, A_prev, Z, W, m, dropout_mask=None, dropout_rate=0.3, lambd=0.0):
    if dropout_mask is not None:
        dA = dA * dropout_mask
        dA /= (1.0 - dropout_rate)

    dZ = dA * leaky_relu_derivative(Z)
    dW = (1/m) * (dZ @ A_prev.T) + (lambd/m) * W  # L2 term added here
    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)
    dA_prev = W.T @ dZ
    return dW, db, dA_prev

def train(X, Y, A0_test, Y_test, learning_rate=0.001, epochs=1000, batch_size=64,
          momentum=0.9, used_dropout=False, dropout_rate=0.2, lambd=0.3):
    global W1, W2, W3, W4, b1, b2, b3, b4

    m = X.shape[1]

    # Initialize momentum terms
    VdW1, VdW2, VdW3, VdW4 = np.zeros_like(W1), np.zeros_like(W2), np.zeros_like(W3), np.zeros_like(W4)
    Vdb1, Vdb2, Vdb3, Vdb4 = np.zeros_like(b1), np.zeros_like(b2), np.zeros_like(b3), np.zeros_like(b4)

    costs = []

    for epoch in range(epochs):
        permutation = np.random.permutation(m)
        X_shuffled = X[:, permutation]
        Y_shuffled = Y[:, permutation]

        for i in range(0, m, batch_size):
            end = i + batch_size
            if end > m:
                end = m

            X_batch = X_shuffled[:, i:end]
            Y_batch = Y_shuffled[:, i:end]
            batch_m = X_batch.shape[1]

            # Forward pass with dropout
            Y_hat, cache = feed_forward(X_batch, training=used_dropout, dropout_rate=dropout_rate)

            # === Backprop ===
            dW4, db4, dA3 = backprop_layer_4(Y_hat, Y_batch, batch_m, cache['A3'], W4, lambd)
            dW3, db3, dA2 = backprop_hidden_layer(dA3, cache['A2'], cache['Z3'], W3, batch_m,
                                                  dropout_mask=cache['D3'] if used_dropout else None,
                                                  dropout_rate=dropout_rate, lambd=lambd)
            dW2, db2, dA1 = backprop_hidden_layer(dA2, cache['A1'], cache['Z2'], W2, batch_m,
                                                  dropout_mask=cache['D2'] if used_dropout else None,
                                                  dropout_rate=dropout_rate, lambd=lambd)
            dW1, db1, _ = backprop_hidden_layer(dA1, X_batch, cache['Z1'], W1, batch_m,
                                                dropout_mask=cache['D1'] if used_dropout else None,
                                                dropout_rate=dropout_rate, lambd=lambd)

            # === Momentum Updates ===
            for W, dW, VdW, name in zip([W4, W3, W2, W1], [dW4, dW3, dW2, dW1], [VdW4, VdW3, VdW2, VdW1], ['W4','W3','W2','W1']):
                VdW[:] = momentum * VdW + (1 - momentum) * dW
                W -= learning_rate * VdW

            for b, db, Vdb, name in zip([b4, b3, b2, b1], [db4, db3, db2, db1], [Vdb4, Vdb3, Vdb2, Vdb1], ['b4','b3','b2','b1']):
                Vdb[:] = momentum * Vdb + (1 - momentum) * db
                b -= learning_rate * Vdb

        # Evaluate training cost and accuracy (no dropout)
        Y_hat_full, _ = feed_forward(X, training=False)
        cost = cross_entropy_loss(Y, Y_hat_full, weights=[W1, W2, W3, W4], lambd=lambd)
        acc = accuracy(Y, Y_hat_full)
        costs.append(cost)
        print(f"Epoch {epoch}: cost = {cost:.6f} | accuracy = {acc:.6f}")

        # Evaluate on test set
        Y_hat_test, _ = feed_forward(A0_test, training=False)
        cost_test = cross_entropy_loss(Y_test, Y_hat_test, weights=[W1, W2, W3, W4], lambd=lambd)
        accuracy_test = accuracy(Y_test, Y_hat_test)
        print(f"Test at {epoch}: cost = {cost_test:.6f} | accuracy = {accuracy_test:.6f}")

    return costs

# @title Load Dataset and Train NN
learning_rate = 0.07 # @param {"type":"number","placeholder":"Learning Rate"}
epochs = 100 # @param {"type":"number","placeholder":"Epochs"}
batch_size = 264 # @param {"type":"number","placeholder":"batch_size"}
momentum = 0.9 # @param {"type":"number","placeholder":"momentum"}
l2_lambda = 0.005 # @param {"type":"number","placeholder":"l2_lambda"}
used_dropout = True # @param{"type":"boolean","placeholder":"used_dropout"}
if apply_dropout==True:
  dropout_rate = 0.35 # @param {"type":"number","placeholder":"dropout_rate"}
else:
  dropout_rate = 0

A0, Y, m, A0_test, Y_test, m_test = prepare_data(split=True)
costs = train(A0, Y, A0_test, Y_test, learning_rate=learning_rate,
              momentum=momentum, epochs=epochs,batch_size=batch_size,
              used_dropout=used_dropout,dropout_rate=dropout_rate,
              lambd=l2_lambda)

# @title Save the Trained Model
import pickle

model_params = {
    'W1': W1,
    'W2': W2,
    'W3': W3,
    'W4': W4,
    'b1': b1,
    'b2': b2,
    'b3': b3,
    'b4': b4
}

with open('model_params.pkl', 'wb') as file:
    pickle.dump(model_params, file)

print("Model parameters saved to 'model_params.pkl'")

load_model("model_params.pkl")